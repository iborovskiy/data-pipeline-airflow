### Date created
2022-11-10


### Project Title
Udacity course project: Data Pipelines with Airflow


### General Description
A startup called Sparkify has decided that it is time to introduce more automation
and monitoring to their data warehouse ETL pipelines and come to the conclusion
that the best tool to achieve this is Apache Airflow.

The goal of this project is to build an ETL pipeline using Apache Airflow.


In this project, the following tasks were completed:
1. Establishing connection to pre-configured Redshift Cluster using IAM role that has read access to S3
2. Building DAG that executes the following ETL tasks:
   - Loading raw input data from the source S3 bucket into staging tables in Redshift Cluster
   - Transforming staging tables into fact and dimension star schema output tables
   - Running data quality checks on output tables

Full DAG for this ETL pipeline: 

![DAG Tasks](./images/example-dag.png)


### Credits

**This entire project is based on learning materials from Udacity:**
https://learn.udacity.com/

The song dataset is a subset of real data from the Million Song Dataset:
http://millionsongdataset.com/

The log dataset consists of log files in JSON format generated by the following event simulator:
https://github.com/Interana/eventsim
